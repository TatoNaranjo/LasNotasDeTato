---
date: 2024-12-02
tags:
  - IA
  - LLMs
---
Esta nota provee un vistazo de algunas arquitecturas transformer. Estas arquitecturas incluyen arquitecturas de solo codificación, solo decodificación, y transformers de codificación y decodificación. Empezaremos con GPT-1 y terminaremos con la última familia de Google llamada Gemini.

## GPT-1

GPT-1 (Generative pre-trained transformer version 1) fue un modelo de solo decodificación desarrollado por OpenAI en 2018, fue entrenado con el dataset de BooksCorpus (el cual contiene aproximadamente un billón de palabras) y es capaz de generar texto, traducir idiomas, escribir diferentes tipos de contenido creativo y responder preguntas de una forma informativa. Las principales innovaciones de GPT-1 fueron:

- **Combinar los transformers y el pre entrenamiento no supervisado:** El pre entrenamiento no supervisado es un proceso de entrenar un modelo en un amplio cuerpo de datos sin etiquetar. Luego, los datos supervisados se utilizan para entrenar el modelo para una tarea específica, como una traducción o una clasificación de sentimientos. En palabras simples, la mayoría de modelos de lenguaje fueron entrenados usando un objetivo de aprendizaje supervisado. Esto significa que el modelo estuvo entrenado en un dataset que tuvo datos etiquetados, en donde cada ejemplo tuvo una etiqueta correspondiente. Este enfoque tiene dos limitaciones principales, la primera es que requiere una cantidad enorme de datos etiquetados, lo cual puede ser caro y puede tomar tiempo al momento de recolectarlos. En segundo lugar, el modelo puede generalizarse únicamente a tareas que son similares a las que fue entrenado. La secuencia de aprendizaje semi-supervisado fue uno de los primeros trabajos que mostró que el pre entrenamiento no supervisado seguido de un entrenamiento supervisado era superior que un único entrenamiento supervisado.
  
  El pre entrenamiento no supervisado se encarga de estas limitaciones entrenando el modelo en un cuerpo grande de datos no etiquetados. Estos datos pueden ser recolectados de forma más sencilla y más barata que los datos etiquetados. Adicionalmente, el modelo puede generalizar tareas que son diferentes a las tareas con las que fue entrenado. El dataset de BooksCorpus es un cuerpo de datos sin etiquetar (5GB) que fue utilizado para entrenar el modelo de lenguaje GPT-1. Este dataset contiene al rededor de 7.000 libros sin publicar, lo que provee al modelo una gran cantidad de datos de los que puede aprender. Adicionalmente, el cuerpo contiene grandes cantidades de texto continuo, lo que ayuda a las dependencias a largo plazo del modelo. Sobre todo, el pre entrenamiento no supervisado es una técnica poderos que puede ser usada para entrenar modelos de lenguajes que son más precisos y generalizables que los modelos que son entrenado utilizando únicamente un aprendizaje supervisado.

- **Transformaciones de entradas en función de las tareas**: Hay diferentes tipos de tareas como la vinculación de textos o el responder preguntas, que requieren de una estructura específica. Por ejemplo, la vinculación textual requiere una premisa y una hipótesis, la resolución de preguntas requiere un documento de contesto: una pregunta y posibles respuestas. Una de las contribuciones de GPT-1 es convertir estos tipos de tareas que requieren entradas estructuradas, en un input que el modelo de lenguaje puede unir sin requerir alguna arquitectura para la tarea específica en la parte superior de la arquitectura pre entrenada. Para la vinculación textual, la premisa `p` y la hipótesis `h` son concatenadas en medio con un token que cumple la labor de delimitar ($) [p, $, h]. Para la resolución de preguntas, el documento de contexto `c` es concatenado con la pregunta `q` y una posible respuesta `a` con un token delimitador en medio de la pregunta y la respuesta [c, q, $, a]/

GPT-1 sobrepasó a modelos previos en muchas pruebas, alcanzando excelentes resultados. Aunque que GPT-1 fue un parte aguas importante en el procesamiento del lenguaje natural (NLP), tuvo algunas limitaciones. Por ejemplo, el modelo generaba texto repetitivo, especialmente cuando se le daban entradas que se salen de sus datos de entrenamiento. También falló en generar múltiples turnos de dialogo y no llevar el registro de las dependencias de texto a largo plazo. Adicionalmente, su cohesión y fluidez estuvieron limitadas a secuencias de texto cortas, y pasajes de textos más largos carecían de cohesión. A pesar de estas limitaciones, GPT-1 demostró el poder del pre entrenamiento jo supervisado, que yacía en la fundación de modelos más grandes y poderosos basados en la arquitectura transformer.

## BERT

BERT que representa el acrónimo para Encoder Representations from Transformers, destaca sobre los modelos transformers de codificación y decodificación tradicional siendo una arquitectura que solo codifica. En vez de traducir o producir secuencias, BERT se enfoca en entender el contexto profundamente por medio del entrenamiento de un modelo objetivo de lenguaje enmascarado. En esta configuración, se reemplazan palabras aleatorias en una secuencia con un token de [MASCARA], y BERT trata de predecir la palabra original basándose en el contexto en el que se rodea. Otro aspecto innovador del entrenamiento de BERT es la predicción de fallos en la siguiente oración, en donde aprende a determinar si una oración precede de manera lógica a una siguiente. Entrenándose en estos objetivos, BERT captura dependencias de un contexto intrincado, en varias perspectivas de una palabra, y puede discernir la relación entre distintos pares de oraciones. Dichas capacidades hacen que BERT sea especialmente bueno en tareas que requieren de un entendimiento de un lenguaje natural, como la resolución de dudas, el análisis de sentimientos y la inferencia de lenguaje natural, entre otros. Como este es un modelo de solo codificación, BERT no puede generar texto.

## GPT-2

GPT-2 es el sucesor de GPT-1 que fue liberado en 2019 por OpenAI. La innovación principal dde GPT-2 fue un escalamiento directo, con un aumento tanto en su contador de parámetros como del tamaño de los datos usados para su entrenamiento:

- **Datos**: GPT-2 fue entrenado con un conjunto de datos de 40GB llamado WebText, el cual consiste en 45 millones de páginas web de Reddit con una puntuación de Karma de como mínimo 3. Karma es una métrica de medición de posicionamiento utilizada en Reddit, y un valor de 3 significa que todos los posts que se sacaron de allí tuvieron un nivel razonable de calidad.
- **Parámetros:** GPT-2 tuvo 1.5 billones de parámetros, lo que a comparación del modelo anterior, fue mucho más grande. Más parámetros incrementan la capacidad de aprendizaje del modelo. Los autores entrenaron cuatro modelos de lenguaje con 117 millones (Al igual que hicieron con GPT-1), 345 millones, 762 millones y 1.5 Billones de parámetros (Este siendo GPT-2), y hallaron que el modelo con la mayor cantidad de parámetros tuvo un mejor rendimiento en cada tarea subsecuente.

Este escalado resultó en un modelo que fue capaz de generar más texto coherente y realista que GPT-1. Su habilidad de generar respuestas parecidas a las de un ser humano lo hicieron una herramienta útil para tareas de procesamiento del lenguaje natural, como la traducción o la creación de contenido. Específicamente, GPT-2 demostró un rendimiento significativo capturando dependencias a largo plazo y en el razonamiento del sentido común. Así como se desempeñó bastante bien en algunas tareas, no pudo superar los resultados obtenidos anteriormente en tareas como la comprensión del estado del arte, los resúmenes y la traducción.  El logro más significativo de GPT-2 fue su capacidad de aprendizaje cero en algunas tareas. Una transferencia de aprendizaje cero de tareas es la habilidad que tiene el modelo para generalizar una nueva tarea sin siquiera haber sido entrenado para la misma, lo que requiere que el modelo entienda la tarea basándose en la instrucción dada. Por ejemplo, para una tarea de traducción de inglés a alemán, el modelo pudo haber recibido una oración en inglés seguido de la palabra "Alemán" y un prompt (;). El modelo pudo ser capaz de entender que lo que se quería llevar a cabo era una tarea de traducción, y generar la traducción en alemán de la oración en inglés. GPT-2 fue capaz de rendir bien en tareas como la traducción de máquina, los resúmenes de texto y la comprensión lectora sin supervisión explícita.

El estudio ha descubierto que el rendimiento en tareas de cero aprendizaje incrementó de forma logarítmica-lineal a medida de que la capacidad del modelo aumentaba. GPT-2 demostró que el entrenamiento con un conjunto de datos más grande y teniendo más parámetros se podían mejorar las habilidades del modelo para entender tareas y sobrepasar aquellas que tenían que ver con el estado del arte por medio de las configuraciones a las tareas de cero aprendizaje.

## GPT-3/3.5/4

GPT-3 es la tercera iteración del modelo generativo transformer pre entrenado, y representa una evolución significativa desde su predecesor, GPT-2, principalmente en términos de escalado, capacidades y flexibilidad. La diferencia más notable es el tamaño de entrenamiento de GPT-3, abarcando una amplia cantidad de 175 billones de parámetros, comparándose con el modelo más grande de GPT-2 que tuvo 1.5 billones de parámetros.
El incremento de tamaño de este modelo permitió a GPT-3 almacenar y traer de vuelta una mayor cantidad de información, entender instrucciones específicas y generar un texto relevante más coherente en mayores cantidades.

Mientras GPT-2 pudo ajustarse bien para tareas específicas con datos de entrenamiento adicionales, GPT-3 puede entender y ejecutar tareas con solo unos pocos ejemplos, o incluso, aveces sin ningún ejemplo explícito, basándose solamente en la instrucción que se le provee. Estas características de GPT-3 también incluyeron el entendimiento dinámico y habilidades de adaptación, reduciendo esa necesidad de un entrenamiento nuevo para tareas específicas que prevalecía tanto en GPT-2.

Finalmente, el cuerpo de entrenamiento del modelo de GPT-3 condujo a una mejor generalización a través de un mayor rango de tareas. Esto significa que fuera del molde y sin un entrenamiento adicional, GPT-3 mostró un rendimiento mejorado en distintos retos de NLP, como pasar de una traducción a una resolución de preguntas, comparado con GPT-2. También es útil notar el enfoque del lanzamiento entre ambos modelos: Mientras que OpenAI inicialmente reservó la exclusividad de GPT-2 debido a preocupaciones de un posible mal uso, decidieron que GPT-3 estuviese disponible como una API comercial, reflejando tanto su utilidad como el involucramiento organizacional en las instancias de despliegue.

El refinamiento de instrucción fue incluido gracias a instructGPT, una versión de GPT-3 que fue re entrenada por medio del aprendizaje supervisado, usando un conjunto de datos de demostraciones humanas que reflejaban un comportamiento del modelo deseado. Las salidas de este modelo fueron medidas y luego vueltas a entrenar utilizando el aprendizaje reforzado de retroalimentación humana. Esto llevó a unas instrucciones mejoradas que se introdujeron en el modelo. Un modelo de InstructGPT de 1.3 Billones de parámetros mostró mejores respuestas que el modelo de 175 Billones de parámetros de GPT-3. También mostró mejoras en cuanto al tema de la confianza en la información, y reducción en la toxicidad de la misma.

Los modelos de GPT-3.5, incluyendo GPT-3.5 turbo, mejoraron la capacidad de entendimiento y generación de código de GPT-3, así como también fueron optimizados para el dialogo. Siendo capaces de recibir ventanas de contexto de hasta 16385 tokens y pudiendo generar hasta un 4.0% de nuevos tokens.

GPT-4 aumenta a GPT-3.5 como un modelo multimodal grande y capaz de procesar imágenes y entradas de texto para producir salidas de texto. Específicamente, aceptar texto o imágenes como una entrada y generar un texto de salida. Este modelo tiene muchas más capacidades de conocimiento general y razonamiento avanzado. Puede recibir ventanas de contexto de hasta 128.000 tokens y tiene una salida máxima del 4.0% de tokens. GPT-4 ha demostrado una versatilidad notable al momento de resolver tareas complejas a lo largo de campos como la matemática, la programación, la visión, la medicina, la abogacía y la psicología, y todo sin instrucciones demasiado específicas. Su rendimiento a menudo coincide e incluso es mejor que las capacidades humanas, y rinde significativamente mejor que modelos anteriores como GPT-3.5.